
import tensorflow as tf
import pandas as pd
import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, models
import numpy as np
import matplotlib.pyplot as plt

df = pd.read_csv(r"C:\Users\HP\Downloads\archive (14)\IMDB Dataset.csv")

df.head()

texts = df['review'].values
labels = df['sentiment'].map({'positive': 1, 'negative': 0}).values

vocab_size = 10000
max_length = 500

tokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded = pad_sequences(sequences, maxlen=max_length)


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(padded, labels, test_size=0.2, random_state=42)


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense
model = models.Sequential()

model.add(layers.Embedding(10000, 16, input_length=500))

model.add(layers.Flatten())

model.add(layers.Dense(16, activation='relu'))

model.add(layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5, batch_size=512, validation_data=(x_test, y_test))

model.summary()

def predict_sentiment(review):
    seq = tokenizer.texts_to_sequences([review])
    padded_seq = pad_sequences(seq, maxlen=max_length)
    pred = model.predict(padded_seq)[0][0]
    print(f"\nReview: {review}")
    print("Sentiment:", "Positive" if pred > 0.5 else "Negative", f"({pred:.2f})")

predict_sentiment("The movie was fantastic and had an amazing storyline")
predict_sentiment("It was a boring and disappointing film")

